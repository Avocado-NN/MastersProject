################################################################################
12/26/2020

Task 1: Make runs save based on metrics and at the end of the run.

Did not have enough time to complete tests. Planning on doing it
tomorrow.

################################################################################
12/27/2020

Continuing doing checkpoint saved based on metrics, losses, and end.

I obtain the following error:

Traceback (most recent call last):
  File "train.py", line 611, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 440, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 146, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 279, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 66, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 483, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 562, in run_training_epoch
    self.trainer.run_evaluation(test_mode=False)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 610, in run_evaluation
    self.evaluation_loop.on_evaluation_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 109, in on_evaluation_end
    self.trainer.call_hook('on_validation_end', *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 823, in call_hook
    trainer_hook(*args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py", line 177, in on_validation_end
    callback.on_validation_end(self, self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 167, in on_validation_end
    self.save_checkpoint(trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 198, in save_checkpoint
    self._validate_monitor_key(trainer)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 441, in _validate_monitor_key
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: ModelCheckpoint(monitor='val_loss') not found in the returned metrics: 
['checkpoint_on', 'train_mask_loss', 'train_quaternion_loss', 'val_mask_loss', 'val_quaternion_loss', 'mask/dice', 'mask/iou', 'mask/f1', 
 'quaternion/rotation_accuracy', 'quaternion/degree_error_AP_5']. 
HINT: Did you call self.log('val_loss', tensor) in the LightningModule?

Changing the checkpoints to match the elements in the list above. Using 
'checkpoint_on' and 'quaternion/degree_error_AP_5'.

After changing the monitor keywords. Now I obtain the following error:

Traceback (most recent call last):
  File "train.py", line 611, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 440, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 146, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 279, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 66, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 483, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 562, in run_training_epoch
    self.trainer.run_evaluation(test_mode=False)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 610, in run_evaluation
    self.evaluation_loop.on_evaluation_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 109, in on_evaluation_end
    self.trainer.call_hook('on_validation_end', *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 823, in call_hook
    trainer_hook(*args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py", line 177, in on_validation_end
    callback.on_validation_end(self, self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 167, in on_validation_end
    self.save_checkpoint(trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 213, in save_checkpoint
    self._save_top_k_checkpoints(monitor_candidates, trainer, pl_module, epoch, filepath)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 494, in _save_top_k_checkpoints
    self._update_best_and_save(filepath, current, epoch, trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 543, in _update_best_and_save
    self._save_model(filepath, trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 297, in _save_model
    raise ValueError(".save_function() not set")
ValueError: .save_function() not set

Perhaps this error is based on the fact that I am saving the model twice when
considering it twice. I removed the 'checkpoint_on' and simply keep the
'quaternion/degree_error_AP_5'.

Got the same error. Looking for answers online:

https://github.com/PyTorchLightning/pytorch-lightning/issues/4275

Looking into Trainer API to confirm the functionality:

https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#trainer-class-api

################################################################################
12/29/2020

Upgrading PyTorch Lightning from 1.0.0 to 1.1.2 to match the documentation online
and improving the situation regarding the checkpoint.

Moving aggregation of predictions into the forward method of the pose regression
model.

Might need to rename the log metrics and losses to remove the character / to allow
saving checkpoints based on their names. The f"{metric}" does not work when / is
present.

This forum talks about this issue:
https://github.com/PyTorchLightning/pytorch-lightning/issues/4012

Seems like this problem won't get resolved. Therefore, I plan on putting my own
saving within my custom callback. Info on how to save the model can be found 
here:
https://pytorch-lightning.readthedocs.io/en/stable/weights_loading.html#manual-saving

################################################################################
12/30/2020

I am working on implementing the saving checkpoint feature inside my own custom
callback.

Now I have successful managed to save the checkpoints based on metrics. The 
problem now is that the training process is frozen after the validation sanity check.
The following output dump demostrates the conditions before stuck:

Validation sanity check: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.56s/it]qt.qpa.xcb: X server does not support XInput 2
failed to get the current screen resources
Epoch 0:   0%|                                                                                                                                        | 0/8 [00:00<?, ?it/s][W reducer.cpp:346] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (function operator())

https://stackoverflow.com/questions/3443607/how-can-i-tell-where-my-python-script-is-hanging#:~:text=Attach%20breakpoints%20at%20key%20places,the%20code%20where%20it%20hangs.

################################################################################
12/31/2020

If I comment out the section where the custom metric-based checkpoints are saved, 
the training routine does not get stuck. 

It is not only the line of code that saves the checkpoint, it is the entire routine
of comparing the metric and saving the model.

It seems that when the training is done by single GPU, the system does not get locked.
Perhaps when executing the saving of the model, the GPUs get descyhronized and
caused locking.

For some reason the following line creates an issue:

complete_checkpoint_path = trainer.log_dir + '/checkpoints/' + new_checkpoint_fp

However, this works?

complete_checkpoint_path = '/checkpoints/' + new_checkpoint_fp

So something with the trainer.log_dir is the problem. 

Even accessing the log_dir attribute of the trainer within the debugger causes the 
program to freeze even though it is not inside the source code. To resolve this 
issue, I plan on making the run's log directory accessable via the environmental
variables.

With the addition of a environmental variable called 'RUN_LOG_DIR', I managed to
avoid using trainer.log_dir. Now checkpoints are working as expected. 

Git pushing: "Complete checkpoint system"


