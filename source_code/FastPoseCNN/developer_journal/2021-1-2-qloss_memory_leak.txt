Problem: After running development runs, AGG_QLOSS modified to account for not 
detection of objects by using torch.tensor(float('nan)) and not adding the loss
to the overal loss. However, now memory shortgage is present and likely due to 
memory leakage going on within AGG_QLOSS.

Solution:


Information:

Useful discussion: 
https://discuss.pytorch.org/t/difference-between-detach-clone-and-clone-detach/34173

--------------------------------------------------------------------------------
Daily Entries
1/2/2021

Now I am running two long test: 

PW_QLOSS_WITH_XYZ
AGG_QLOSS_WITH_XYZ

################################################################################
1/4/2021

After the runs have been completed, I noticed that AGG_QLOSS_WITH_XYZ is just logging
nan. This is mostly due to that loss becomes torch.tensor([nan]).

Epoch 44: 100%|████████████████████████| 326/326 [02:47<00:00,  1.95it/s, loss=nan, v_num=_]
Epoch 47: 100%|████████████████████████| 326/326 [02:35<00:00,  2.09it/s, loss=nan, v_num=_]
Epoch 47: 100%|████████████████████████| 326/326 [02:47<00:00,  1.95it/s, loss=nan, v_num=_]
Epoch 48: 100%|███████████████████████▌| 325/326 [02:34<00:00,  2.11it/s, loss=nan, v_num=_]
Epoch 58: 100%|███████████████████████▌| 325/326 [02:35<00:00,  2.08it/s, loss=nan, v_num=_]
Epoch 62: 100%|████████████████████████| 326/326 [02:34<00:00,  2.11it/s, loss=nan, v_num=_]
Epoch 67: 100%|███████████████████████▌| 325/326 [02:37<00:00,  2.06it/s, loss=nan, v_num=_]
Epoch 71: 100%|████████████████████████| 326/326 [02:47<00:00,  1.95it/s, loss=nan, v_num=_]
Epoch 74: 100%|████████████████████████| 326/326 [02:41<00:00,  2.01it/s, loss=nan, v_num=_]
Epoch 74: 100%|████████████████████████| 326/326 [02:46<00:00,  1.96it/s, loss=nan, v_num=_]

Additionally, the AGG_QLOSS_WITH_XYZ mostly flatlines throughout the trainining, which
is very weird. This makes sense when you look at the visualization. No predictions.
The problem might be that when the model is first learning how to perform segmentation,
the aggregated QLoss function is shifting the gradients too gravely to allow for
training. To fix this, when there is no objects to aggregated and perform AGG_QLOSS,
instead of placing a standard value and then stacking that with the other losses, 
we should remove this loss for that epoch.

All the logs in the degree_error_AP_5/epoch is simply NaN for AGG_QLOSS (makes sense) 
and 0 for PW_QLOSS (doesn't make sense). The rotation_accuracy/epoch metric is 
extremely high for PW_QLOSS which is not good. Need to look into that.

TODO: 
1.) Need to make quaternions, xy, and z not destroy the gradient when there is no 
intersection between the ground truth and predicted mask. This means that the 
quaternion, xy, and z losses should be ignored when there is no intersection.

2.) Fix PW_QLOSS, something probably with the background.

Obtaining this really strange bug: 

python train.py -e=AGG_QLOSS_NAN_IMPLEMENT
No protocol specified
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3]
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [2,3]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2

  | Name  | Type          | Params
----------------------------------------
0 | model | PoseRegressor | 16.8 M
----------------------------------------
16.8 M    Trainable params
0         Non-trainable params
16.8 M    Total params
Validation sanity check: 100%|██████████████████████████| 2/2 [00:04<00:00,  3.02s/it]qt.qpa.xcb: X server does not support XInput 2
failed to get the current screen resources
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 0:   0%|                                                                                                                                                                               | 0/326 [00:00<?, ?it/s][W reducer.cpp:346] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 64, 1, 1], strides() = [64, 1, 64, 64]
bucket_view.sizes() = [256, 64, 1, 1], strides() = [64, 1, 1, 1] (function operator())
Epoch 0:   7%|█████████▊          | 22/326 [00:28<06:35,  1.30s/it, loss=14.7, v_num=_]
Saving latest checkpoint...
Epoch 0:   7%|█████████▊          | 22/326 [00:30<06:58,  1.38s/it, loss=14.7, v_num=_]
Traceback (most recent call last):
  File "train.py", line 626, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 307, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 521, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 560, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 718, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 493, in optimizer_step
    model_ref.optimizer_step(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1258, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 278, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 136, in __optimizer_step
    optimizer.step(closure=closure, *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/catalyst/contrib/nn/optimizers/lookahead.py", line 61, in step
    loss = self.optimizer.step(closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/catalyst/contrib/nn/optimizers/radam.py", line 55, in step
    loss = closure()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 708, in train_step_and_backward_closure
    result = self.training_step_and_backward(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 806, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 330, in training_step
    training_step_output = self.trainer.accelerator_backend.training_step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 158, in training_step
    return self._step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 172, in _step
    output = self.trainer.model(*args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py", line 179, in forward
    output = self.module.training_step(*inputs[0], **kwargs[0])
  File "train.py", line 125, in training_step
    multi_task_losses, multi_task_metrics = self.shared_step('train', batch, batch_idx)
  File "train.py", line 205, in shared_step
    losses, metrics = self.loss_function(
  File "train.py", line 238, in loss_function
    losses[loss_name] = loss_attrs['F'](outputs, inputs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/lib/loss.py", line 82, in forward
    return loss(y_pred, y_true)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_toolbelt/losses/focal.py", line 86, in forward
    cls_label_target = cls_label_target[not_ignored]
RuntimeError: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 11.78 GiB total capacity; 10.15 GiB already allocated; 47.75 MiB free; 10.31 GiB reserved in total by PyTorch)

It seems like the model is being saved during training, causing a large memory
burden for the GPUs. I tried commenting out the section in the custom callback 
where I save the model. Perhaps it is the build-in checkpoint callback of PyTorch
lightning. 

It still crashes. As much as I don't want to, I am going to decrease the batch
size.

Got this error:

Traceback (most recent call last):
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 626, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 307, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 492, in train
    self.run_sanity_check(self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 690, in run_sanity_check
    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 622, in run_evaluation
    deprecated_eval_results = self.evaluation_loop.evaluation_epoch_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 208, in evaluation_epoch_end
    deprecated_results = self.__run_eval_epoch_end(self.num_dataloaders, using_eval_result)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 260, in __run_eval_epoch_end
    eval_results = self.__auto_reduce_result_objs(outputs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 291, in __auto_reduce_result_objs
    result = result.__class__.reduce_on_epoch_end(dl_output)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py", line 519, in reduce_on_epoch_end
    recursive_stack(result)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py", line 660, in recursive_stack
    result[k] = collate_tensors(v)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py", line 682, in collate_tensors
    return torch.stack(items)
RuntimeError: All input tensors must be on the same device. Received cpu and cuda:1

Now I need to determine which tensor is in the cpu to .cuda() it. It has something
to do with the dataloaders. I changed nothing but now it works :| ?

However, the stupid 'saving lastest checkpoint' error is still causing the entire 
thing to fail >:(. Going to remove the automatic checkpoint of pytorch lightning.

After removing the checkpoint, 

Traceback (most recent call last):
  File "train.py", line 629, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 68, in train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 521, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 560, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 718, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 493, in optimizer_step
    model_ref.optimizer_step(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1258, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 278, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 136, in __optimizer_step
    optimizer.step(closure=closure, *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/catalyst/contrib/nn/optimizers/lookahead.py", line 61, in step
    loss = self.optimizer.step(closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/catalyst/contrib/nn/optimizers/radam.py", line 55, in step
    loss = closure()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 708, in train_step_and_backward_closure
    result = self.training_step_and_backward(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 806, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 330, in training_step
    training_step_output = self.trainer.accelerator_backend.training_step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 83, in training_step
    return self._step(self.trainer.model.training_step, args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 78, in _step
    output = model_step(*args)
  File "train.py", line 125, in training_step
    multi_task_losses, multi_task_metrics = self.shared_step('train', batch, batch_idx)
  File "train.py", line 205, in shared_step
    losses, metrics = self.loss_function(
  File "train.py", line 258, in loss_function
    metrics[metric_name] = metric_attrs['F'](pred, gt)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/classification.py", line 919, in f1_score
    return __f1(preds=pred, target=target, num_classes=num_classes, average=class_reduction)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/f_beta.py", line 153, in f1
    return fbeta(preds, target, num_classes, 1.0, threshold, average, multilabel)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/f_beta.py", line 103, in fbeta
    true_positives, predicted_positives, actual_positives = _fbeta_update(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/f_beta.py", line 28, in _fbeta_update
    preds, target = _input_format_classification_one_hot(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/utils.py", line 112, in _input_format_classification_one_hot
    return preds.reshape(num_classes, -1), target.reshape(num_classes, -1)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 11.78 GiB total capacity; 10.36 GiB already allocated; 13.75 MiB free; 10.39 GiB reserved in total by PyTorch)

the error is still present. That is weird.

By using loss_mse for the quaternion, I avoid having any errors during training.
Placed back the checkpoints. That is not the problem. I believe the problem is 
based on the aggreagated QLoss function. Using a single GPU.

git pushed: "stable version with loss_mse"

Now trying to debug aggreated QLoss. Got the following error:

Traceback (most recent call last):
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 632, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 307, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 492, in train
    self.run_sanity_check(self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 690, in run_sanity_check
    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 622, in run_evaluation
    deprecated_eval_results = self.evaluation_loop.evaluation_epoch_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 208, in evaluation_epoch_end
    deprecated_results = self.__run_eval_epoch_end(self.num_dataloaders, using_eval_result)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 260, in __run_eval_epoch_end
    eval_results = self.__auto_reduce_result_objs(outputs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 291, in __auto_reduce_result_objs
    result = result.__class__.reduce_on_epoch_end(dl_output)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py", line 519, in reduce_on_epoch_end
    recursive_stack(result)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py", line 660, in recursive_stack
    result[k] = collate_tensors(v)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py", line 682, in collate_tensors
    return torch.stack(items)
RuntimeError: All input tensors must be on the same device. Received cpu and cuda:1

Found information regarding this issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/4877


Now got this following error:

  File "train.py", line 630, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 307, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 492, in train
    self.run_sanity_check(self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 690, in run_sanity_check
    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 625, in run_evaluation
    self.evaluation_loop.on_evaluation_epoch_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 334, in on_evaluation_epoch_end
    self.trainer.call_hook('on_validation_epoch_end', *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 887, in call_hook
    trainer_hook(*args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py", line 87, in on_validation_epoch_end
    callback.on_validation_epoch_end(self, self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/callbacks.py", line 69, in on_validation_epoch_end
    self.shared_epoch_end('valid', trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/callbacks.py", line 128, in shared_epoch_end
    self.log_epoch_average(mode, trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/callbacks.py", line 167, in log_epoch_average
    average = torch.mean(torch.stack(cuda_0_log))
RuntimeError: Can only calculate the mean of floating types. Got Long instead.

I placed .float() in places where I use 'nan's. Now I get this error:

Traceback (most recent call last):
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 630, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 307, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 521, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 588, in run_training_epoch
    self.trainer.run_evaluation(test_mode=False)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 606, in run_evaluation
    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 178, in evaluation_step
    output = self.trainer.accelerator_backend.validation_step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 161, in validation_step
    return self._step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 172, in _step
    output = self.trainer.model(*args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py", line 185, in forward
    output = self.module.validation_step(*inputs[0], **kwargs[0])
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 151, in validation_step
    multi_task_losses, multi_task_metrics = self.shared_step('valid', batch, batch_idx)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 178, in shared_step
    outputs = self.model(batch['image'])
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/lib/pose_regressor.py", line 133, in forward
    cc_quaternion = gtf.class_compress(
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/lib/gpu_tensor_funcs.py", line 50, in class_compress
    class_data = torch.unsqueeze(class_mask, dim=1) * class_data[object_class_id-1]
IndexError: index 1 is out of bounds for dimension 0 with size 1
Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:13<00:00,  2.13it/s, loss=5.97, v_num=_]
Traceback (most recent call last):
  File "train.py", line 630, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 307, in ddp_train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 521, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 588, in run_training_epoch
    self.trainer.run_evaluation(test_mode=False)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 606, in run_evaluation
    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 178, in evaluation_step
    output = self.trainer.accelerator_backend.validation_step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 161, in validation_step
    return self._step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 172, in _step
    output = self.trainer.model(*args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py", line 185, in forward
    output = self.module.validation_step(*inputs[0], **kwargs[0])
  File "train.py", line 151, in validation_step
    multi_task_losses, multi_task_metrics = self.shared_step('valid', batch, batch_idx)
  File "train.py", line 178, in shared_step
    outputs = self.model(batch['image'])
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/lib/pose_regressor.py", line 133, in forward
    cc_quaternion = gtf.class_compress(
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/lib/gpu_tensor_funcs.py", line 50, in class_compress
    class_data = torch.unsqueeze(class_mask, dim=1) * class_data[object_class_id-1]
IndexError: index 1 is out of bounds for dimension 0 with size 1

Then I obtain this error:

Traceback (most recent call last):
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 521, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 560, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 718, in run_training_batch
    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 493, in optimizer_step
    model_ref.optimizer_step(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1258, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 278, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 136, in __optimizer_step
    optimizer.step(closure=closure, *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/catalyst/contrib/nn/optimizers/lookahead.py", line 61, in step
    loss = self.optimizer.step(closure)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/catalyst/contrib/nn/optimizers/radam.py", line 55, in step
    loss = closure()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 708, in train_step_and_backward_closure
    result = self.training_step_and_backward(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 806, in training_step_and_backward
    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 330, in training_step
    training_step_output = self.trainer.accelerator_backend.training_step(args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 83, in training_step
    return self._step(self.trainer.model.training_step, args)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 78, in _step
    output = model_step(*args)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 125, in training_step
    multi_task_losses, multi_task_metrics = self.shared_step('train', batch, batch_idx)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 205, in shared_step
    losses, metrics = self.loss_function(
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 258, in loss_function
    metrics[metric_name] = metric_attrs['F'](pred, gt)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/classification.py", line 919, in f1_score
    return __f1(preds=pred, target=target, num_classes=num_classes, average=class_reduction)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/f_beta.py", line 153, in f1
    return fbeta(preds, target, num_classes, 1.0, threshold, average, multilabel)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/f_beta.py", line 103, in fbeta
    true_positives, predicted_positives, actual_positives = _fbeta_update(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/functional/f_beta.py", line 28, in _fbeta_update
    preds, target = _input_format_classification_one_hot(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/utils.py", line 101, in _input_format_classification_one_hot
    target = to_onehot(target, num_classes=num_classes)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/metrics/utils.py", line 141, in to_onehot
    tensor_onehot = torch.zeros(
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 11.78 GiB total capacity; 10.37 GiB already allocated; 5.75 MiB free; 10.40 GiB reserved in total by PyTorch)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/students/edavalos/.vscode-server/extensions/ms-python.python-2020.12.424452561/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/students/edavalos/.vscode-server/extensions/ms-python.python-2020.12.424452561/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/students/edavalos/.vscode-server/extensions/ms-python.python-2020.12.424452561/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/usr/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/usr/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/train.py", line 630, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 68, in train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 552, in train
    self.train_loop.on_train_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 191, in on_train_end
    self.check_checkpoint_callback(should_save=True, is_last=True)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 225, in check_checkpoint_callback
    callback.on_validation_end(self.trainer, model)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 203, in on_validation_end
    self.save_checkpoint(trainer, pl_module)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 248, in save_checkpoint
    self._save_top_k_checkpoints(trainer, pl_module, monitor_candidates)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 590, in _save_top_k_checkpoints
    self._update_best_and_save(current, epoch, step, trainer, pl_module, metrics)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 619, in _update_best_and_save
    filepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 523, in _get_metric_interpolated_filepath_name
    filepath = self.format_checkpoint_name(epoch, step, ckpt_name_metrics)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 439, in format_checkpoint_name
    filename = self._format_checkpoint_name(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 407, in _format_checkpoint_name
    filename = filename.format(**metrics)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/torch/tensor.py", line 535, in __format__
    return object.__format__(self, format_spec)
TypeError: unsupported format string passed to Tensor.__format__


Traceback (most recent call last):
  File "train.py", line 632, in <module>
    trainer.fit(
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 68, in train
    results = self.train_or_test()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 69, in train_or_test
    results = self.trainer.train()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 521, in train
    self.train_loop.run_training_epoch()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 588, in run_training_epoch
    self.trainer.run_evaluation(test_mode=False)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 625, in run_evaluation
    self.evaluation_loop.on_evaluation_epoch_end()
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 334, in on_evaluation_epoch_end
    self.trainer.call_hook('on_validation_epoch_end', *args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 887, in call_hook
    trainer_hook(*args, **kwargs)
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py", line 87, in on_validation_epoch_end
    callback.on_validation_epoch_end(self, self.get_model())
  File "/home/students/edavalos/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/callbacks.py", line 102, in on_validation_epoch_end
    os.remove(monitor_data['saved_checkpoint_fp'])
FileNotFoundError: [Errno 2] No such file or directory: '/home/students/edavalos/GitHub/FastPoseCNN/source_code/FastPoseCNN/logs/21-01-04/16-42-DEUBG-NOCS-resnet18-imagenet/_/checkpoints/epoch=1--quaternion_degree_error_AP_5=0.00.ckpt'

Fixing the FileNotFoundError by using a simple try, except since it is not crucial

Likely what is happening is a memory leak within aggreated QLoss.

https://discuss.pytorch.org/t/memory-leak-debugging-and-common-causes/67339

Quote on how to debug memory leakage

"""
the most useful way I found to debug is to use torch.cuda.memory_allocated() and 
torch.cuda.max_memory_allocated() to print a percent of used memory at the top of 
the training loop. Then look at your training loop, add a continue statement right 
below the first line and run the training loop. If your memory usage holds steady, 
move the continue to the next line and so on until you find the leak.
"""

It seems like PW_QLOSS and MSE_LOSS are stable and functional. The only problem
AQQ_QLOSS.

I fix the memory leak when I make the AQQ_QLOSS function return early. There is 
a section in AQQ_QLOSS that causes the memory to leak.

Created a run to test MSE with XYZ regression called:

MSE_WITH_XYZ

################################################################################
1/5/2021

With the completion of MSE run, the model slightly performs better than PW_QLOSS.
Additionally, the MSE run achieves good degree_error_AP_5/epoch with obtaining 
a good value of 4.363 (training). Note that the validation routine just has Nans
however in the batch version there are plenty of time that the metric obtains 
non Nans, therefore implying that something is wrong in the calculation of average
for the epoch. The rotation_accuracy converges to 30, which is bad but good in 
the sense that the value converges to a smaller quantity.

Goal for today is to just make AGG QLOSS stable to run, then will fix QLOSS
performance.

I detached the tensors when logged by the custom logger I created. After making these
changes, I was able to train while using aggreagated QLoss for 10 epochs, with 
training size of 5000 and validation size of 200 (single GPU).

Now mask segmentation is working well, but quaternion regression is still pretty
bad. However, it seems like the logging of metrics is not working well with Nans.

To handle this, I plan on removing all the nans before taking the epoch average.
To avoid stacking an empty list: If all are Nan, then simple use Nan for the epoch.

Doing this helped ensure that the plotting and logging of epoch averages are accurate
and account for visualization valuable information. 

git pushed: "stable, fixed memory leak, nan dominated epoch logs, and partially gradient
wipeout due to high quaternion loss."